{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fe7a8bf5490>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "np.random.seed(0)\n",
    "torch.random.manual_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Difference from tensorflow implementation(so far)\n",
    "1. Time step is not input to neural net\n",
    "2. Initial state is initialized randomly and learnable, instead of zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_left_prob(held_out_range = [[0.1, 0.2], [0.3, 0.4]]):\n",
    "    def check_heldout(left_prob):\n",
    "        inrange = False\n",
    "        for low, high in held_out_range:\n",
    "            if left_prob>low and left_prob<high:\n",
    "                inrange = True\n",
    "        return inrange\n",
    "    left_prob = -1\n",
    "    while left_prob<0 or left_prob>0.5 or check_heldout(left_prob):\n",
    "        left_prob = np.random.uniform(0, 0.5)\n",
    "    return left_prob\n",
    "\n",
    "def get_trial(current_state, left_prob, action): # get next trial after action\n",
    "    assert 0<=left_prob and left_prob<=0.5, \"probability out of range\"\n",
    "    right_prob = 0.5-left_prob\n",
    "    current_state = current_state.copy()\n",
    "    current_state[action] = 0\n",
    "    if np.random.uniform()<left_prob:\n",
    "        current_state[0] = 1\n",
    "    if np.random.uniform()<right_prob:\n",
    "        current_state[1] = 1\n",
    "    return current_state\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "minimum = 30\n",
    "for i in range(10000):\n",
    "    minimum = min(minimum, np.random.uniform())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class A3C(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.init_h = torch.nn.Parameter(torch.zeros(1, 48))\n",
    "        self.init_c = torch.nn.Parameter(torch.zeros(1, 48))\n",
    "        self.lstm = torch.nn.LSTMCell(3, 48)\n",
    "        self.action_head = torch.nn.Linear(48, 2)\n",
    "        self.value_head = torch.nn.Linear(48, 1)\n",
    "    def forward(self, prev_action, prev_reward, hidden):\n",
    "        input_vec = torch.zeros(3)\n",
    "        if prev_action!=-1: # if not first trial\n",
    "            input_vec[prev_action] = 1\n",
    "        input_vector[2] = prev_reward\n",
    "        if hidden == None:\n",
    "            lstm_out = self.lstm(input_vector[None, ...], (self.init_h, self.init_c))\n",
    "        else:\n",
    "            lstm_out = self.lstm(input_vector[None, ...], hidden)\n",
    "        action = self.action_head(lstm_out)\n",
    "        value = self.value_head(lstm_out)\n",
    "        return action, value\n",
    "\n",
    "agent = A3C()\n",
    "optimizer = torch.optim.RMSprop(agent.parameters(), 0.007)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-61-1992b7a26b06>, line 13)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-61-1992b7a26b06>\"\u001b[0;36m, line \u001b[0;32m13\u001b[0m\n\u001b[0;31m    value_preds.\u001b[0m\n\u001b[0m                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def run_block(train = True):\n",
    "    num_trials = np.random.randint(50, 101)\n",
    "    left_prob = generate_left_prob()\n",
    "    initial_state = get_trial([0, 0], left_prob, 0)\n",
    "    action = -1\n",
    "    reward = 0\n",
    "    hidden = None\n",
    "    value_preds = []\n",
    "    rewards = []\n",
    "    actions = []\n",
    "    for trial_num in range(num_trials):\n",
    "        action_prop, value_pred = agent(action, reward, hidden)\n",
    "        value_preds.append(value_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
